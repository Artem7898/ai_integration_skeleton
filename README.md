# AI Integration Skeleton (OpenAI / Hugging Face / LangChain) 

–ì–æ—Ç–æ–≤—ã–π –∫–∞—Ä–∫–∞—Å –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö AI‚Äë–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π –Ω–∞ **FastAPI** —Å —É–ø–æ—Ä–æ–º –Ω–∞:
- **Structured Outputs** –∏ **tool calling** (OpenAI)
- **RAG** –Ω–∞ –±–∞–∑–µ **LangChain** (—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ HF, –ª–æ–∫–∞–ª—å–Ω–∞—è –≤–µ–∫—Ç–æ—Ä–∫–∞)
- –í–∞—Ä–∏–∞–Ω—Ç—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: **–æ–±–ª–∞–∫–æ (HF Inference Endpoints)** –∏ **–ª–æ–∫–∞–ª—å–Ω–æ (`transformers`)**
- –ü–æ–ª–∏—Ç–∏–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: —Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è, –ø–µ—Å–æ—á–Ω–∏—Ü–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, egress‚Äëallowlist
- –ì–æ—Ç–æ–≤—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –∑–∞–ø—É—Å–∫–∞: `uvicorn`, `Dockerfile`, `docker-compose`, `Kubernetes` (+ `NetworkPolicy` –¥–ª—è egress‚Äëallowlist)

- ## üìå –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- **–ï–¥–∏–Ω—ã–π API** –ø–æ–≤–µ—Ä—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ LLM/Embeddings.
- **–≠–Ω–¥–ø–æ–∏–Ω—Ç /ask** —Å –ø–æ—Ç–æ–∫–æ–≤–æ–π –≤—ã–¥–∞—á–µ–π (SSE/Chunked), –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –∑–∞—â–∏—Ç–∞–º–∏.
- **Rate limiting, Idempotency-Key, retries, circuit breaker.**
- **JWT** —Å —Ä–æ–ª—è–º–∏/—Å–∫–æ—É–ø–∞–º–∏; CORS, CSRF (–¥–ª—è cookie‚Äë—Ä–µ–∂–∏–º–∞).
- **–ö—ç—à** –æ—Ç–≤–µ—Ç–æ–≤ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Redis).
- **–û—á–µ—Ä–µ–¥–∏** –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –∑–∞–¥–∞—á (Celery/Redis).
- **RFC 7807** —Ñ–æ—Ä–º–∞—Ç –æ—à–∏–±–æ–∫, **ETag/Cache-Control**, **cursor‚Äëpagination**.
- **Observability**: –ª–æ–≥–∏ JSON, Prometheus –º–µ—Ç—Ä–∏–∫–∏, OpenTelemetry —Ç—Ä–∞—Å—Å—ã.


---

## 0. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

- Python 3.10+
- –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
  - `OPENAI_API_KEY` ‚Äî –∫–ª—é—á OpenAI (—Ö—Ä–∞–Ω–∏—Ç—å –¢–û–õ–¨–ö–û –Ω–∞ –±—ç–∫–µ)
  - `HF_TOKEN` ‚Äî —Ç–æ–∫–µ–Ω Hugging Face (–¥–ª—è InferenceClient/—ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤)
  - `LOG_LEVEL` ‚Äî —É—Ä–æ–≤–µ–Ω—å –ª–æ–≥–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é `INFO`)

–°–æ–∑–¥–∞–π `.env` (–ª–æ–∫–∞–ª—å–Ω–æ –∏/–∏–ª–∏ –¥–ª—è Docker Compose):

```env
OPENAI_API_KEY=sk-...
HF_TOKEN=hf_...
LOG_LEVEL=INFO
```

---

## 1. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
ai_integration_skeleton/
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ main.py                  # FastAPI-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (—ç–Ω–¥–ø–æ–∏–Ω—Ç /ask, –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã)
‚îÇ  ‚îú‚îÄ ai_openai.py             # OpenAI: structured outputs + tool calling
‚îÇ  ‚îú‚îÄ ai_hf_cloud.py           # Hugging Face InferenceClient (–æ–±–ª–∞–∫–æ)
‚îÇ  ‚îú‚îÄ ai_hf_local.py           # Hugging Face transformers (–ª–æ–∫–∞–ª—å–Ω–æ)
‚îÇ  ‚îú‚îÄ rag_chain.py             # LangChain RAG (JSON Output + –ø–∞—Ä—Å–µ—Ä)
‚îÇ  ‚îú‚îÄ safe_tools.py            # –î–µ–∫–æ—Ä–∞—Ç–æ—Ä safe_tool (–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø–æ–ª–∏—Ç–∏–∫–∏)
‚îÇ  ‚îî‚îÄ settings.py              # –ó–∞–≥—Ä—É–∑–∫–∞ .env, –∫–æ–Ω—Ñ–∏–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
‚îú‚îÄ requirements.txt
‚îú‚îÄ Dockerfile
‚îú‚îÄ docker-compose.yml
‚îú‚îÄ k8s/
‚îÇ  ‚îú‚îÄ deployment.yaml
‚îÇ  ‚îú‚îÄ service.yaml
‚îÇ  ‚îú‚îÄ configmap.yaml
‚îÇ  ‚îú‚îÄ secret.yaml
‚îÇ  ‚îî‚îÄ networkpolicy-egress-allowlist.yaml
‚îî‚îÄ README.md 
```

–£—Å—Ç–∞–Ω–æ–≤–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash
pip install -r requirements.txt
```

–ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ (dev):
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8001
```

---

## 2. –ö–æ–¥: –±–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏

### 2.1 `app/settings.py` ‚Äî –∫–æ–Ω—Ñ–∏–≥/–ª–æ–≥–∏
```python
import os
from functools import lru_cache
from pydantic import BaseModel
from dotenv import load_dotenv
import logging

load_dotenv()

class Settings(BaseModel):
    openai_api_key: str | None = os.getenv("OPENAI_API_KEY")
    hf_token: str | None = os.getenv("HF_TOKEN")
    log_level: str = os.getenv("LOG_LEVEL", "INFO")

@lru_cache
def get_settings() -> Settings:
    return Settings()

def setup_logging(level: str):
    logging.basicConfig(
        level=getattr(logging, level.upper(), logging.INFO),
        format="%(asctime)s %(levelname)s %(name)s %(message)s"
    )
```

### 2.2 `app/ai_openai.py` ‚Äî Structured Outputs + tool calling
```python
import os
from pydantic import BaseModel, Field, ValidationError
from openai import OpenAI

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

class FlightSearch(BaseModel):
    origin: str = Field(..., min_length=3, max_length=64)
    destination: str = Field(..., min_length=3, max_length=64)
    date: str = Field(..., pattern=r"^\d{4}-\d{2}-\d{2}$")  # YYYY-MM-DD

SCHEMA = {
    "type": "json_schema",
    "json_schema": {
        "name": "flight_query",
        "schema": FlightSearch.model_json_schema()
    }
}

def search_flights(origin: str, destination: str, date: str):
    return [{"origin": origin, "destination": destination, "date": date, "price": 1234}]

TOOLS = [{
    "type": "function",
    "function": {
        "name": "search_flights",
        "description": "Search real flights safely",
        "parameters": FlightSearch.model_json_schema()
    }
}]

def ask_openai_flights(query: str):
    resp = client.responses.create(
        model="gpt-4.1-mini",
        input=query,
        response_format=SCHEMA,
        tools=TOOLS,
    )
    calls = []
    for item in (resp.output or []):
        for piece in (item.content or []):
            if getattr(piece, "type", None) == "tool_call" and piece.tool.name == "search_flights":
                try:
                    args = FlightSearch(**piece.tool.arguments)
                except ValidationError as e:
                    raise ValueError(f"Bad tool args: {e}")
                calls.append(search_flights(args.origin, args.destination, args.date))
    return calls
```

### 2.3 `app/ai_hf_cloud.py` ‚Äî HF InferenceClient
```python
import os
from huggingface_hub import InferenceClient

client = InferenceClient(
    model="meta-llama/Meta-Llama-3-8B-Instruct",
    token=os.environ.get("HF_TOKEN")
)

def hf_generate_json(prompt: str, max_new_tokens: int = 128) -> str:
    return client.text_generation(prompt, max_new_tokens=max_new_tokens, details=False)
```

### 2.4 `app/ai_hf_local.py` ‚Äî –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ transformers
```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

MODEL_ID = "Qwen/Qwen2.5-0.5B-Instruct"
_tok = AutoTokenizer.from_pretrained(MODEL_ID)
_mdl = AutoModelForCausalLM.from_pretrained(MODEL_ID)
_pipe = pipeline("text-generation", model=_mdl, tokenizer=_tok)

def local_generate(prompt: str, max_new_tokens: int = 64) -> str:
    return _pipe(prompt, max_new_tokens=max_new_tokens, temperature=0.2)[0]["generated_text"]
```

### 2.5 `app/rag_chain.py` ‚Äî LangChain RAG (JSON Output)
```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langchain_openai import ChatOpenAI

prompt = ChatPromptTemplate.from_messages([
  ("system", "–¢—ã –æ—Ç–≤–µ—á–∞–µ—à—å —Å—Ç—Ä–æ–≥–æ –≤ JSON: {format}"),
  ("user", "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {question}\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}")
])

parser = JsonOutputParser()
chain = (prompt | ChatOpenAI(model="gpt-4o-mini", temperature=0) | parser)

def rag_answer(question: str, context: str) -> dict:
    return chain.invoke({"question": question, "context": context, "format": parser.get_format_instructions()})
```

### 2.6 `app/safe_tools.py` ‚Äî –ø—Ä–∏–º–µ—Ä –ø–µ—Å–æ—á–Ω–∏—Ü—ã
```python
from functools import wraps
from pydantic import BaseModel, ValidationError

def safe_tool(schema: type[BaseModel]):
    def deco(fn):
        def wrapper(**kwargs):
            try:
                args = schema(**kwargs)
            except ValidationError as e:
                raise ValueError(f"Bad args: {e}")
            if hasattr(args, "url"):
                allow = ("https://api.example.com/", "https://internal.service/")
                if not str(args.url).startswith(allow):
                    raise PermissionError("URL not allowed")
            return fn(**args.dict())
        return wrapper
    return deco
```

### 2.7 `app/main.py` ‚Äî FastAPI –∫–∞—Ä–∫–∞—Å
```python
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI
import os, json
from .settings import get_settings, setup_logging

settings = get_settings()
setup_logging(settings.log_level)

app = FastAPI(title="AI Integration Skeleton", version="0.1.0")

client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])

class Ask(BaseModel):
    question: str

@app.post("/ask")
def ask(body: Ask):
    schema = { "type":"json_schema","json_schema":{"name":"answer","schema":{
        "type":"object","properties":{"answer":{"type":"string"}},"required":["answer"]
    }}}
    resp = client.responses.create(
        model="gpt-4.1-mini",
        input=f"–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ —Å—Ç—Ä–æ–≥–æ –≤ JSON –ø–æ —Å—Ö–µ–º–µ: –≤–æ–ø—Ä–æ—Å: {body.question}",
        response_format=schema,
    )
    raw = resp.output[0].content[0].text
    payload = json.loads(raw)
    assert isinstance(payload, dict) and "answer" in payload
    return payload
```

---

## 3. –ó–∞–ø—É—Å–∫: Uvicorn

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000
```

---

## 4. Docker

### 4.1 `requirements.txt`
```txt
fastapi==0.112.2
uvicorn[standard]==0.30.6
python-dotenv==1.0.1
pydantic==2.8.2
openai==1.40.6
huggingface_hub==0.25.1
transformers==4.44.2
langchain==0.2.14
langchain-openai==0.1.22
```

### 4.2 `Dockerfile`
```Dockerfile
FROM python:3.12-slim

ENV PYTHONDONTWRITEBYTECODE=1     PYTHONUNBUFFERED=1

WORKDIR /app

RUN pip install --no-cache-dir --upgrade pip
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

COPY app ./app

EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 4.3 `docker-compose.yml`
```yaml
version: "3.9"
services:
  ai-service:
    build: .
    image: ai-integration:latest
    ports:
      - "8000:8000"
    environment:
      OPENAI_API_KEY: "${OPENAI_API_KEY}"
      HF_TOKEN: "${HF_TOKEN}"
      LOG_LEVEL: "${LOG_LEVEL:-INFO}"
    restart: unless-stopped
```

–ó–∞–ø—É—Å–∫:
```bash
docker compose up --build -d
```

---

## 5. Kubernetes (—Å egress‚Äëallowlist)

### 5.1 `k8s/configmap.yaml`
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-config
data:
  LOG_LEVEL: "INFO"
```

### 5.2 `k8s/secret.yaml`
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: ai-secrets
type: Opaque
stringData:
  OPENAI_API_KEY: "replace_me"
  HF_TOKEN: "replace_me"
```

### 5.3 `k8s/deployment.yaml`
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-integration
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-integration
  template:
    metadata:
      labels:
        app: ai-integration
    spec:
      containers:
        - name: api
          image: ai-integration:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
          envFrom:
            - configMapRef:
                name: ai-config
            - secretRef:
                name: ai-secrets
          resources:
            requests:
              cpu: "250m"
              memory: "256Mi"
            limits:
              cpu: "1"
              memory: "1Gi"
```

### 5.4 `k8s/service.yaml`
```yaml
apiVersion: v1
kind: Service
metadata:
  name: ai-integration-svc
spec:
  type: ClusterIP
  selector:
    app: ai-integration
  ports:
    - name: http
      port: 80
      targetPort: 8000
```

### 5.5 `k8s/networkpolicy-egress-allowlist.yaml`
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ai-egress-allowlist
spec:
  podSelector:
    matchLabels:
      app: ai-integration
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
```
